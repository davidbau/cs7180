<html>

<head>
  <style>
    body {
      margin: 0.5in;
      background-color: aliceblue
    }

    h1,
    h2,
    h3,
    h4,
    h5 {
      font-family: helvetica, arial, sans-serif
    }

    .cal {
      border-collapse: collapse;
    }

    .cal td {
      border-bottom: 1px solid black;
      vertical-align: top;
      padding: 3px 5px;
    }
  </style>
</head>

<h1>CS 7180 - Fall 2024 - The Structure and Interpretation of Deep Networks</h1>

<p>Tuesdays 11:45 AM - 1:25 PM and Thursdays 2:50 PM - 4:30 PM in Ell Hall 410.</p>
<p>This course will be built around a review of the current literature in
   interpretable deep networks and deep network explanation methods (also known as 
   "explainable AI," "interpretable AI," or "mechanistic interpretability").  Topics may include:

<ul>
    <li>Model representations - "How do deep networks represent information?"</li>
    <li>Internal computation - "What do models do with these internal representations?"</li>
    <li>Learning dynamics - "What happens to model representations during training? How do they learn?"</li>
    <li>Understanding the world - "To what extent do deep networks build models of the world? How can we make them more robust?"</li>
</ul>

<p>Prerequisites: CS 7150 or equivalent. You must be familiar with
    training, fine-tuning, and using deep networks with pytorch, including
    use in both natural language (transformers, RNNs, LLMs) and computer vision
    settings (CNNs, GANs, VAEs, Diffusion models). You must be
    comfortable with machine learning, linear algebra, gradient optimization
    methods, and probability and statistics.

<p>The course will be designed for research-oriented grad students.
    Because it is centered around student presentations, interactive demonstrations,
    and discussions, it will be held in-person only. 
<!-- <ul>
    <li>Model representations - probing, neurons, subspaces, SAEs</li>
    <li>Internal computation - circuits, factual association, induction heads</li>
    <li>Learning dynamics - grokking, data attribution, in-context learning, unlearning</li>
    <li>Understanding the world - world models, safety, steering, automated scientists </li>
</ul> -->

<!-- <ul>
  <li>Salience methods: answering "what is the network looking at?"
  <li>Representation probing methods: "what is the network looking for?"
  <li>Causal methods: "what makes the network behave as it does?"
  <li>Model editing: "can we directly edit model parameters?"
</ul> -->

<h2>Course Structure</h2>

<p><b>The course schedule can be found <a href="">here</a>.</b> Every class, 2-3 students will be 
    responsible for leading a discussion centered around the papers listed on the schedule. For that day,
    discussion leaders will be responsible for: 

    <!-- (a) reading the papers for the day before everybody else
    (b) creating a one-page webpage/latex "explanation" of the beautiful idea and state-of-the-science for the papers
    (c) creating "a googleform of reading group questions" to post to the class the day before, to drive the discussion -->
<ol>
    <li>Reading the main papers for that day, as well as any other supplementary articles/links provided</li>
    <li>Creating a one-page webpage/latex "explainer" of the beautiful idea and state-of-the-science for the papers</li>
    <li>Posting three discussion questions on Canvas for other students to respond to before class</li>
</ol>

<h3>What do you mean by "explainer" in (2)?</h3>
<p>The original inspiration for this idea was the <a href="https://people.maths.ox.ac.uk/trefethen/pdectb.html">PDE coffee table book</a>, 
which was a group project at Oxford with the goal of creating a beautiful book describing different partial differential equations. 

<p>For our purposes, you can think of it as creating a single page in a "handbook" of interpretability topics, 
with more practical workaday tips on how to get going on a topic, such as: 
<ul>
    <li>a brief explainer of the math you actually need to understand</li>
    <li>opinionated pointers on what paragraph to read in which paper for what perspective</li>
    <li>opinionated pointers to github or huggingface repositories containing code or data or models with notes on what quality they have</li>
    <li>an example piece of running code (demos are always great)!</li>
</ul>

<h2>Final Project</h2>

<p>For your final project, you will be asked to apply model interpretation 
methods to a problem setting relevant to your own research interests. You will need to create
a website/latex report on your research, similar to the ones created for class discussions. 

<p>At the end of the semester we will have a workshop where
we invite all the non-computer scientists to join and
see all the final project demonstrations.

<h2>Grading</h2>
You will need to help run two classes throughout the semester, each of which will 
count for 20% of your grade (40% total). 30% of your grade will be based on class participation 
(e.g., reading the papers as evidenced by filling out the forms of questions requested by your classmates, 
coming to class and offering thoughts), and the last 30% will come from your final project. 